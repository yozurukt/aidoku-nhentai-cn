import os
import re
from urllib.request import urlopen, Request

# nhentai requires User-Agent
USER_AGENT = "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) GSA/300.0.598994205 Mobile/15E148 Safari/604"


def extract_tags(html: str) -> list[tuple[str, int]]:
    """Parse tags from nhentai HTML page."""
    tags: list[tuple[str, int]] = []
    for m in re.finditer(r'<a[^>]+href="(/tag/[^"]+)"[^>]*>(.*?)</a>', html, re.DOTALL):
        a_html: str = m.group(0)
        # Extract tag name
        name_match = re.search(r'<span[^>]*class="name"[^>]*>(.*?)</span>', a_html)
        if name_match:
            name: str = name_match.group(1).strip()
        else:
            name = re.sub(r"<.*?>", "", m.group(2)).strip()
        # Extract count
        count_match = re.search(r'<span[^>]*class="count"[^>]*>(.*?)</span>', a_html)
        if count_match:
            count_text: str = count_match.group(1).strip().replace(",", "")
            if count_text.endswith("K"):
                count: int = int(float(count_text[:-1]) * 1000)
            elif count_text.endswith("M"):
                count = int(float(count_text[:-1]) * 1000000)
            else:
                try:
                    count = int(count_text)
                except ValueError:
                    count = 0
        else:
            count = 0
        if count >= 10:
            tags.append((name, count))
    return tags


def fetch_all_tags() -> list[str]:
    """Fetch all tags from nhentai."""
    tags: list[tuple[str, int]] = []
    page = 1
    while True:
        url = f"https://nhentai.net/tags/popular?page={page}"
        print(f"Fetching page {page}...")
        req: Request = Request(url, headers={"User-Agent": USER_AGENT})
        try:
            with urlopen(req) as response:
                html = response.read().decode("utf-8")
        except Exception as e:
            print(f"Error fetching page {page}: {e}")
            break
        page_tags = extract_tags(html)
        if not page_tags:
            break
        tags.extend(page_tags)
        page += 1
        if page > 100:
            break

    # Keep original popularity order (most popular first)
    return [name for name, _ in tags]


def escape_rust_string(s: str) -> str:
    """Escape a string for use in Rust source code."""
    return s.replace('\\', '\\\\').replace('"', '\\"')


def generate_rust_file(tags: list[str]) -> str:
    """Generate Rust source file with tags array."""
    lines = [
        "// This file is auto-generated by update_tags.py",
        "// Do not edit manually!",
        "",
        "#![allow(clippy::unreadable_literal)]",
        "",
        "/// All available tags from nhentai (English)",
        "pub static TAGS_EN: &[&str] = &[",
    ]
    
    for tag in tags:
        escaped = escape_rust_string(tag)
        lines.append(f'    "{escaped}",')
    
    lines.append("];")
    return "\n".join(lines)


def save_rust_file(content: str, filename: str):
    """Save generated Rust file to src directory."""
    base_dir = os.path.dirname(os.path.realpath(__file__))
    src_dir = os.path.join(base_dir, "..", "src")
    
    file_path = os.path.join(src_dir, filename)
    
    print(f"Saving to {file_path}...")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"Done. Generated {len(content)} bytes.")


if __name__ == "__main__":
    tags = fetch_all_tags()
    print(f"Fetched {len(tags)} tags")
    
    rust_code = generate_rust_file(tags)
    save_rust_file(rust_code, "tags.rs")
